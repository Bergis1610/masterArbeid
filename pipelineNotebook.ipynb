{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image as img\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BRYGGE_SEKVENS = \"./bilder/brygge_sekvens\"\n",
    "BRO_SEKVENS = \"./bilder/bro_sekvens\"\n",
    "LAGRA_BILDER = \".bilder/lagra_bilete\"\n",
    "BLANDA_SEKVENS = \"./bilder/blanda_sekvens\"\n",
    "RESULT_FOLDER = BRYGGE_SEKVENS\n",
    "\n",
    "K = np.loadtxt(f\"{RESULT_FOLDER}/left/K_matrix.txt\")\n",
    "R = np.loadtxt(f\"{RESULT_FOLDER}/left/R_matrix.txt\")\n",
    "T = np.loadtxt(f\"{RESULT_FOLDER}/left/T_matrix.txt\")\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# Under åpner vi ti = n'te bildet\n",
    "left_images_filenames = list(filter(lambda fn: fn.split(\".\")[-1]==\"png\", os.listdir(f\"{RESULT_FOLDER}/left\")))\n",
    "timestamps = list(map(lambda fn: fn.split(\".\")[0], left_images_filenames))\n",
    "ti = 0\n",
    "timestamp = timestamps[ti]\n",
    "left = cv2.imread(f\"{RESULT_FOLDER}/left/{timestamp}.png\")\n",
    "right = cv2.imread(f\"{RESULT_FOLDER}/right/{timestamp}.png\")\n",
    "disp = np.array(cv2.imread(f\"{RESULT_FOLDER}/disp_zed/{timestamp}.png\", cv2.IMREAD_ANYDEPTH) / 256.0, dtype=np.float32)\n",
    "\n",
    "print(\"Første:\" ,timestamps[0], \" Nærme: \", timestamps[len(timestamps)-1])\n",
    "\n",
    "def fetch_image(main_as_well=False, RES_FOLDER=RESULT_FOLDER, time=timestamp):\n",
    "\n",
    "    plt.ion()\n",
    "    new_main_image = cv2.imread(f\"{RES_FOLDER}/left/{time}.png\")\n",
    "    new_disparity_image = np.array(cv2.imread(f\"{RES_FOLDER}/disp_zed/{time}.png\", cv2.IMREAD_ANYDEPTH) / 256.0, dtype=np.float32)\n",
    "    if(main_as_well):\n",
    "        return new_disparity_image, new_main_image\n",
    "    return new_disparity_image\n",
    "\n",
    "def display_disp_image(to_be_displayed, title=\"Image\", d_type=\"turbo\"):\n",
    "    plt.imshow(to_be_displayed, cmap=d_type)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Timestamps\n",
    "# Første: 1689072599961543541  Nærme:  1689072633388130541 Bru:  1689068851836122093 Båt: 1689068854381001093\n",
    "Første= 1689072599961543541  \n",
    "Nærme=  1689072633388130541\n",
    "Bru =  1689068851836122093\n",
    "Båt =  1689068854381001093\n",
    "\n",
    "# Fra blanda\n",
    "Kajakk1 = 1689068879500907093\n",
    "KajakkOgBåt = 1689068879500907093\n",
    "KajakkBåtTett = 1689068866505573093\n",
    "BåtBak = 1689068856993399093\n",
    "BåtFram = 1689068871596675093\n",
    "BaderingMann = 1689072623607882541\n",
    "Badering = 1689072630042776541\n",
    "BaderingMannMidten = 1689072625349756541\n",
    "\n",
    " \n",
    "\n",
    "# Fetch a new current working image and respective disparity\n",
    "# Dette er disp_første og venstre_første\n",
    "#current_working_disparity, current_working_image = fetch_image(True)\n",
    "\n",
    "# Dette er disp_nærme_brygga og nærme_brygga\n",
    "current_working_disparity, current_working_image = fetch_image(True, RES_FOLDER=BRYGGE_SEKVENS ,time=Første)\n",
    "første_disparity, første_image = fetch_image(True, RES_FOLDER=BRYGGE_SEKVENS ,time=Første)\n",
    "nærme_disparity, nærme_image = fetch_image(True, RES_FOLDER=BRYGGE_SEKVENS, time=Nærme)\n",
    "bru_disparity, bru_image = fetch_image(True, RES_FOLDER=BRO_SEKVENS, time=Bru)\n",
    "båt_disparity, båt_image = fetch_image(True, RES_FOLDER=BRO_SEKVENS, time=Båt)\n",
    "\n",
    "kajakk_første_disparity, kajakk_første_image = fetch_image(True, RES_FOLDER=BLANDA_SEKVENS ,time=Kajakk1)\n",
    "kajakk_båt_disparity, kajakk_båt_image = fetch_image(True, RES_FOLDER=BLANDA_SEKVENS ,time=KajakkOgBåt)\n",
    "kajakk_tett_disparity, kajakk_tett_image = fetch_image(True, RES_FOLDER=BLANDA_SEKVENS ,time=KajakkBåtTett)\n",
    "båt_bak_disparity, båt_bak_image = fetch_image(True, RES_FOLDER=BLANDA_SEKVENS ,time=BåtBak)\n",
    "båt_fram_disparity, båt_fram_image = fetch_image(True, RES_FOLDER=BLANDA_SEKVENS ,time=BåtFram)\n",
    "badering_første_disparity, badering_første_image = fetch_image(True, RES_FOLDER=BLANDA_SEKVENS ,time=Badering)\n",
    "badering_mann_disparity, badering_mann_image = fetch_image(True, RES_FOLDER=BLANDA_SEKVENS ,time=BaderingMann)\n",
    "badering_midten_disparity, badering_midten_image = fetch_image(True, RES_FOLDER=BLANDA_SEKVENS ,time=BaderingMannMidten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def interpolate_column(column):\n",
    "    # Indices of valid and invalid elements\n",
    "    # Adds the indices of values that are valid meaning real numbers to valid_indices and invalid indices meaning inf, -inf and NaN to \n",
    "    valid_mask = np.isfinite(column)\n",
    "    valid_indices = np.where(valid_mask)[0]\n",
    "    invalid_indices = np.where(~valid_mask)[0]\n",
    "\n",
    "    # Check if we have enough data for interpolation\n",
    "    if len(valid_indices) == 0:\n",
    "        # No valid data in this column\n",
    "        return column\n",
    "    elif len(invalid_indices) == 0:\n",
    "        # No need for interpolation\n",
    "        return column\n",
    "\n",
    "    # Interpolate invalid data points\n",
    "    valid_data = column[valid_mask]\n",
    "    column[~valid_mask] = np.interp(invalid_indices, valid_indices, valid_data)\n",
    "    return column\n",
    "\n",
    "def interpolate_each_column(dI):\n",
    "    # Applying the interpolation to each column\n",
    "    height, width = dI.shape\n",
    "    for x in range(width):\n",
    "        dI[:, x] = interpolate_column(dI[:, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n = 15  # Size of the kernel, as Vipul used in his thesis 5\n",
    "kernel_k = np.ones(n) / n  # Kernel for averaging\n",
    "\n",
    "# Gaussian kernel:\n",
    "n_gaussian = 15  # Size of the kernel\n",
    "sigma_kernel = 2.0  # Standard deviation which controls the spread\n",
    "\n",
    "def gaussian_kernel(n_gaussian=n_gaussian, sigma=sigma_kernel): \n",
    "    # Create a coordinate vector from -n//2 to n//2\n",
    "    x = np.arange(-n_gaussian//2 + 1, n_gaussian//2 + 1)\n",
    "\n",
    "    # Calculate the Gaussian function\n",
    "    kernel_g = np.exp(-0.5 * (x**2) / sigma**2)\n",
    "\n",
    "    # Normalize the kernel to ensure the sum of kernel elements equals 1\n",
    "    kernel_g /= np.sum(kernel_g)\n",
    "\n",
    "    return kernel_g\n",
    "\n",
    "kernel_gaussian = gaussian_kernel()\n",
    "\n",
    "# Old binomial kernel\n",
    "\"\"\" n_binomial = 15\n",
    "\n",
    "def binomial_kernel(size = n_binomial):\n",
    "    kernel_b = np.array([1, 1])\n",
    "    for i in range(size - 2):\n",
    "        kernel_b = np.convolve(kernel_b, np.array([1, 1]))\n",
    "    kernel_b = kernel_b.astype(float)\n",
    "    kernel_b /= np.sum(kernel_b)  # Normalize the kernel\n",
    "    return kernel_b\n",
    "\n",
    "kernel_binomial = binomial_kernel() \"\"\"\n",
    "from scipy.special import binom\n",
    "\n",
    "n = 30  # Length of the kernel\n",
    "kernel_binomial = np.array([binom(n-1, k) for k in range(n)])\n",
    "kernel_binomial = kernel_binomial / np.sum(kernel_binomial) \n",
    "\n",
    "# Velg input Kernel!\n",
    "# Endre/change/velg\n",
    "#-------------------------------------------\n",
    "#input_kernel_lowpass = kernel_k\n",
    "#kern_bool = True\n",
    "input_kernel_lowpass = kernel_binomial\n",
    "kern_bool = False\n",
    "#-------------------------------------------\n",
    "\n",
    "\n",
    "# Function to apply low-pass filter to a column\n",
    "def low_pass_filter(column, K=kernel_k):\n",
    "    # Apply convolution\n",
    "    filtered_column = np.convolve(column, K, mode='same')\n",
    "    return filtered_column\n",
    "\n",
    "#Endring av input, TELEPORTER hit\n",
    "#-------------------------------------------\n",
    "# Endre/change/velg\n",
    "# Velg disparity bilde her!\n",
    "#-------------------------------------------\n",
    "#current_working_disparity, current_working_image \n",
    "#første_disparity, første_image \n",
    "#nærme_disparity, nærme_image \n",
    "#bru_disparity, bru_image \n",
    "#båt_disparity, båt_image\n",
    "#\n",
    "#dI = bru_disparity\n",
    "#original_image = bru_image\n",
    "\n",
    "#dI = båt_disparity\n",
    "#original_image = båt_image\n",
    "\n",
    "#dI = første_disparity\n",
    "#original_image = første_image\n",
    "\n",
    "#dI = nærme_disparity\n",
    "#original_image = nærme_image\n",
    "\n",
    "#dI = kajakk_første_disparity\n",
    "#original_image = kajakk_første_image\n",
    "\n",
    "#dI = kajakk_båt_disparity\n",
    "#original_image = kajakk_båt_image\n",
    "\n",
    "#dI = kajakk_tett_disparity\n",
    "#original_image = kajakk_tett_image\n",
    "\n",
    "dI = båt_bak_disparity\n",
    "original_image = båt_bak_image\n",
    "\n",
    "#dI = båt_fram_disparity\n",
    "#original_image = båt_fram_image\n",
    "\n",
    "#dI = badering_første_disparity\n",
    "#original_image = badering_første_image\n",
    "\n",
    "#dI = badering_mann_disparity\n",
    "#original_image = badering_mann_image\n",
    "\n",
    "#dI = badering_midten_disparity\n",
    "#original_image = badering_midten_image\n",
    "\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "interpolate_each_column(dI)\n",
    "\n",
    "# This is manually doing the above cell\n",
    "# Apply the low-pass filter to each column\n",
    "height, width = dI.shape\n",
    "df = np.zeros_like(dI)  # Low-pass filtered disparity image\n",
    "for x in range(width):\n",
    "    df[:, x] = low_pass_filter(dI[:, x], K=input_kernel_lowpass)\n",
    "\n",
    "#df = cv2.filter2D(dI, -1, input_kernel_lowpass.reshape(-1, 1))  # Reshape for vertical application\n",
    "\n",
    "# Versjon 1 bruker sobel Kernel til å derivere\n",
    "# Define the 1D Sobel kernelS\n",
    "kernel_m = np.array([1, 0, -1])\n",
    "\n",
    "# Initialize an array to store the vertical derivative image\n",
    "vertical_derivative = np.zeros_like(df)\n",
    "\n",
    "# Apply the Sobel kernel to each column\n",
    "\"\"\" for i in range(df.shape[1]):  # Iterate over columns\n",
    "    vertical_derivative[:, i] = np.convolve(df[:, i], kernel_m, mode='same') \"\"\"\n",
    "\n",
    "for i in range(df.shape[1]):  # Iterate over columns\n",
    "    vertical_derivative[:, i] = np.convolve(df[:, i], kernel_m, mode='same') / 2\n",
    "\n",
    "\n",
    "\n",
    "# Versjon 2 bruker np.diff \n",
    "# Computing the derivative of each column\n",
    "df_derivative = np.diff(df, axis=0)\n",
    "\n",
    "# Padding to handle size difference after np.diff\n",
    "df_derivative = np.pad(df_derivative, ((0, 1), (0, 0)), mode='edge')\n",
    "#første_df_derivative = np.pad(df_derivative, ((0, 1), (0, 0)), mode='edge') \n",
    "\n",
    "\n",
    "\n",
    "# Input should be a disparity image where each column is differentiated\n",
    "def flatten_and_plot_histogram(df_derivative, plot=True,title=\"Histogram of Derivative Values\", input_bins=2000):\n",
    "    # Flatten the array of derivatives to a 1D array\n",
    "    flattened_derivatives = df_derivative.flatten()\n",
    "\n",
    "    if(plot):\n",
    "        # Plot the histogram\n",
    "        plt.figure()\n",
    "        plt.hist(flattened_derivatives, bins=input_bins, range=(-0.3,0.5), color='blue', edgecolor='blue')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Derivative Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    \n",
    "    return flattened_derivatives\n",
    "    \n",
    "\n",
    "# Versjon 1 Sobel\n",
    "flattened_derivatives_sobel = flatten_and_plot_histogram(vertical_derivative, title=\"Sobel Method histogram\")\n",
    "\n",
    "# Versjon 2 np.diff\n",
    "flattened_derivatives_diff = flatten_and_plot_histogram(df_derivative, title=\"Diff method histogram\")\n",
    "\n",
    "#flattened_derivatives = flattened_derivatives_sobel\n",
    "\n",
    "flattened_derivatives = flattened_derivatives_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian(x, mean, stddev):\n",
    "    return (1 / (stddev * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / stddev) ** 2)\n",
    "\n",
    "# Gaussian fit function\n",
    "def fit_gaussian(data):\n",
    "    mean = np.mean(data)\n",
    "    stddev = np.std(data)\n",
    "    return mean, stddev\n",
    "\n",
    "def clustering(flattened_derivatives, guess1=[0.0], guess2=[0.06], a_mini=-0.3, a_maxi=0.5, clusters=2):\n",
    "    # Clipping the data to be between -0.3 and 0.5\n",
    "    clipped_derivatives = np.clip(flattened_derivatives, a_min=a_mini, a_max=a_maxi)\n",
    "\n",
    "    # Reshape data for KMeans\n",
    "    clustering_model = clipped_derivatives.reshape(-1, 1)\n",
    "    \n",
    "    # Initial guesses for cluster centers (approximated from histogram peaks)\n",
    "    initial_centers = np.array([guess1, guess2])  # replace peak1, peak2 with your estimates\n",
    "\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=clusters, init=initial_centers, n_init=1)\n",
    "    kmeans.fit(clustering_model)\n",
    "\n",
    "    # Get the labels and cluster centers\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    return clustering_model, clipped_derivatives, labels, centers\n",
    "\n",
    "def clusteringV2(flattened_derivatives, a_mini=-0.3, a_maxi=0.5, clusters=2):\n",
    "    # Clipping the data to be between -0.3 and 0.5\n",
    "    clipped_derivatives = np.clip(flattened_derivatives, a_min=a_mini, a_max=a_maxi)\n",
    "\n",
    "    # Reshape data for KMeans\n",
    "    clustering_model = clipped_derivatives.reshape(-1, 1)\n",
    "\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=clusters, init='k-means++', n_init=10)\n",
    "    kmeans.fit(clustering_model)\n",
    "\n",
    "    # Get the labels and cluster centers\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    return clustering_model, clipped_derivatives, labels, centers\n",
    "\n",
    "# Sobel versjon\n",
    "#clustering_model, labels, centers = clustering(flattened_derivatives_sobel, guess1=[0.0], guess2=[0.15])\n",
    "# Diff\n",
    "clustering_model, clipped_derivatives, labels, centers = clusteringV2(flattened_derivatives_diff)\n",
    "\n",
    "# Relocation the mean to match the mode for a more accurate distribution\n",
    "\n",
    "cluster1 = clustering_model[labels == 0]\n",
    "cluster2 = clustering_model[labels == 1]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(cluster1, bins=200, range=(-0.2, 0.4), density=True, alpha=0.6)\n",
    "plt.xlabel('Derivative Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Cluster 1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram \n",
    "plt.figure()\n",
    "plt.hist(cluster2, bins=200, range=(-0.2, 0.4), density=True, alpha=0.6)\n",
    "plt.xlabel('Derivative Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Cluster 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(clustering_model, bins=200, range=(-0.2, 0.4), density=True, alpha=0.6)\n",
    "plt.xlabel('Derivative Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram with Fitted Gaussian Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mode_result = stats.mode(cluster2)\n",
    "print(type(mode_result.mode))\n",
    "\n",
    "print(\"mode: \", stats.mode(cluster2).mode[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit Gaussian to each cluster\n",
    "mean1, stddev1 = fit_gaussian(cluster1)\n",
    "mean2, stddev2 = fit_gaussian(cluster2)\n",
    "#stddev1, stddev2 = 0.02, 0.02\n",
    "print(\"Mean 1: \",mean1)\n",
    "print(\"Stddev 1: \",stddev1)\n",
    "print(\"Mean 2: \",mean2)\n",
    "print(\"Stddev 2: \",stddev2)\n",
    "print(\"Mode 1: \", stats.mode(cluster1).mode[0])\n",
    "print(\"Mode 2: \", stats.mode(cluster2).mode[0])\n",
    "\n",
    "if(kern_bool):\n",
    "    mean1 = stats.mode(cluster1).mode[0]\n",
    "    mean2 = stats.mode(cluster2).mode[0]\n",
    "\n",
    "\n",
    "# Determine which mean is closer to 0 and assign labels\n",
    "if abs(mean1) < abs(mean2):\n",
    "    red_mean, green_mean = mean1, mean2\n",
    "    red_stddev, green_stddev = stddev1, stddev2\n",
    "    red_label, green_label = 'Cluster 1 (Red)', 'Cluster 2 (Green)'\n",
    "else:\n",
    "    red_mean, green_mean = mean2, mean1\n",
    "    red_stddev, green_stddev = stddev2, stddev1\n",
    "    red_label, green_label = 'Cluster 2 (Red)', 'Cluster 1 (Green)'\n",
    "\n",
    "# Below are the upper and lower bounds\n",
    "# Assuming you have mean1, stddev1, mean2, stddev2 from the Gaussian fits\n",
    "k = 1 # Adjust this constant as needed\n",
    "\n",
    "\"\"\" shift_amount = 0.02  # Adjust this value to shift the curve to the left\n",
    "# Adjust mean2 for Gaussian Curve 2\n",
    "adjusted_mean2 = mean2 - shift_amount\n",
    "mean2 = mean2 - shift_amount \n",
    "#gaussian_curve2Adjust = gaussian(x_values, adjusted_mean2, stddev2)\n",
    "\"\"\"\n",
    "\n",
    "# Calculate range limits for each Gaussian curve\n",
    "dL1 = mean1 - k * stddev1\n",
    "dU1 = mean1 + k * stddev1\n",
    "dL2 = mean2 - k * stddev2\n",
    "dU2 = mean2 + k * stddev2\n",
    "\n",
    "red_dL = red_mean - k * red_stddev\n",
    "red_dU = red_mean + k * red_stddev\n",
    "green_dL = green_mean - k * green_stddev\n",
    "green_dU = green_mean + k * green_stddev\n",
    "\n",
    "\n",
    "# Create a range of x values\n",
    "x_values = np.linspace(-0.3, 0.5, 400)\n",
    "\n",
    "\n",
    "\n",
    "# Gaussian curves\n",
    "gaussian_curve1 = gaussian(x_values, mean1, stddev1)\n",
    "gaussian_curve2 = gaussian(x_values, mean2, stddev2)\n",
    "\n",
    "\n",
    "# Plot histogram and Gaussian curves\n",
    "plt.figure()\n",
    "plt.hist(clipped_derivatives, bins=200, range=(-0.2, 0.4), density=True, alpha=0.6)\n",
    "plt.plot(x_values, gaussian_curve1, label='Gaussian 1')\n",
    "plt.plot(x_values, gaussian_curve2, label='Gaussian 2')\n",
    "plt.xlabel('Derivative Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram with Fitted Gaussian Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And dL1, dU1, dL2, dU2 are the range limits for the two Gaussians\n",
    "classification_map = np.empty(df_derivative.shape, dtype=object)\n",
    "\n",
    "include_undecided_label = True\n",
    "\n",
    "if include_undecided_label:\n",
    "    print(\"includes undecided\")\n",
    "    for i in range(df_derivative.shape[0]):  # Rows\n",
    "        for j in range(df_derivative.shape[1]):  # Columns\n",
    "            derivative_value = df_derivative[i, j]\n",
    "\n",
    "            # Check if the derivative value falls into the range of either Gaussian\n",
    "            if (red_dL <= derivative_value <= red_dU) and (green_dL <= derivative_value <= green_dU):\n",
    "                classification = 'undecided'\n",
    "            elif red_dL <= derivative_value <= red_dU:\n",
    "                classification = 'upright'  # Red cluster\n",
    "            elif green_dL <= derivative_value <= green_dU:\n",
    "                classification = 'horizontal'  # Green cluster\n",
    "            else:\n",
    "                classification = 'unknown'\n",
    "\n",
    "            classification_map[i, j] = classification\n",
    "else:\n",
    "    print(\"Undecided and unknown is the same\")\n",
    "    for i in range(df_derivative.shape[0]):  # Rows\n",
    "        for j in range(df_derivative.shape[1]):  # Columns\n",
    "            derivative_value = df_derivative[i, j]\n",
    "\n",
    "            # Check if the derivative value falls into the range of either Gaussian\n",
    "            if (red_dL <= derivative_value <= red_dU) and (green_dL <= derivative_value <= green_dU):\n",
    "                classification = 'unknown'\n",
    "            elif red_dL <= derivative_value <= red_dU:\n",
    "                classification = 'upright'  # Red cluster\n",
    "            elif green_dL <= derivative_value <= green_dU:\n",
    "                classification = 'horizontal'  # Green cluster\n",
    "            else:\n",
    "                classification = 'unknown'\n",
    "\n",
    "            classification_map[i, j] = classification\n",
    "\n",
    "print(df_derivative)\n",
    "print(classification_map)\n",
    "\n",
    "# Dark color definitions with brightness < 128/256\n",
    "# Brightness = 0.18\n",
    "Navy_Blue = [0, 0, 128/256]  # Brightness = 0.17\n",
    "Dark_Slate_Gray = [47/256, 79/256, 79/256]  # Brightness = 0.32\n",
    "Dark_Cyan = [0, 139/256, 139/256]  # Brightness = 0.18\n",
    "Dark_Goldenrod = [184/256, 134/256, 11/256]  # Adjusted to be darker\n",
    "White= [1,1,1]\n",
    "\n",
    "# Revised dark, distinct color definitions\n",
    "Dark_Red = [139/256, 0, 0]  # Rich, dark red\n",
    "Dark_Slate_Gray = [47/256, 79/256, 79/256]\n",
    "Sienna = [160/256, 82/256, 45/256]  # Earthy, medium dark\n",
    "Dark_Orange = [255/256, 140/256, 0]  # Vibrant, but darkened\n",
    "Saddle_Brown = [139/256, 69/256, 19/256] \n",
    "Pure_Red =[200/256,0,0]\n",
    "\n",
    "# Horizontal color definitions\n",
    "Indigo = [75/256, 0, 130/256]  # Deep indigo blue\n",
    "Teal = [0, 128/256, 128/256]  # Rich teal blue\n",
    "Forest_Green = [34/256, 139/256, 34/256]  # Deep forest green\n",
    "Dark_Olive_Green = [85/256, 107/256, 47/256]  # Muted, darker green\n",
    "Midnight_Blue = [25/256, 25/256, 112/256] \n",
    "Dark_Green = [0, 100/256, 0]  \n",
    "Pure_Green = [0,200/256,0]\n",
    "Pure_Blue = [0,0,200/256]\n",
    "\n",
    "\n",
    "# Updated color scheme for classification ensuring maximum distinction\n",
    "colorScheme = {\n",
    "    'horizontal1': Navy_Blue,\n",
    "    'horizontal2': Teal,\n",
    "    'horizontal3': Pure_Green,\n",
    "    'horizontal4': Indigo,\n",
    "    'horizontal5': Dark_Olive_Green,\n",
    "    'horizontal6': Pure_Green,  # Custom darker teal\n",
    "    'horizontal7': Dark_Green,  # Custom dark leaf green\n",
    "    'vertical1': Dark_Red,\n",
    "    'vertical2': Pure_Red,\n",
    "    'vertical3': Sienna,\n",
    "    'vertical4': Dark_Orange,\n",
    "    'vertical5': Dark_Slate_Gray,  # Custom burnt orange\n",
    "    'unknown': White,  # White\n",
    "    'undecided': White,  # White\n",
    "    'upright': Pure_Red,\n",
    "    'horizontal': Pure_Green\n",
    "}\n",
    "\n",
    "\n",
    "def assign_colours_and_plot(classification_map, colors=colorScheme, plot=True, title=\"Pixel-wise Disparity Map Classification\"):\n",
    "    # Create an empty array for the color-coded image\n",
    "    color_coded_image = np.zeros((*classification_map.shape, 3))\n",
    "\n",
    "    # Assign colors\n",
    "    for classification, color in colors.items():\n",
    "        mask = classification_map == classification\n",
    "        color_coded_image[mask] = color\n",
    "\n",
    "    if(plot):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(color_coded_image)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')  # Hide the axes\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Nærme\n",
    "#display_disp_image(nærme_image)\n",
    "# Båt\n",
    "#display_disp_image(båt_test_bilde)\n",
    "assign_colours_and_plot(classification_map, title=\"Horizontal and vertical pixel segmentation\")\n",
    "display_disp_image(original_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_splitting_disparity_image = dI\n",
    "\n",
    "vertCount = 0\n",
    "horCount = 0\n",
    "noneCount = 0\n",
    "\n",
    "# Verticals\n",
    "vertical_Derivative_List = []\n",
    "vertical_Disp_List = []\n",
    "vertical_Obj_List = []\n",
    "\n",
    "\n",
    "#Horizonals\n",
    "horizontal_Derivative_List = []\n",
    "horizontal_Disp_List = []\n",
    "horizontal_Obj_List = []\n",
    "\n",
    "divided_horizontal_obj_list = []\n",
    "\n",
    "\n",
    "for i in range(classification_map.shape[0]):  # Rows\n",
    "\n",
    "    single_col_obj_list = []\n",
    "    for j in range(classification_map.shape[1]):  # Columns\n",
    "\n",
    "        current_local_value = classification_map[i, j]\n",
    "\n",
    "        if(current_local_value == 'upright'):\n",
    "            vertCount += 1\n",
    "            vertical_Derivative_List.append(df_derivative[i,j])\n",
    "            appendObjectVert = [surface_splitting_disparity_image[i, j], i, j, 'vertical']\n",
    "            vertical_Disp_List.append(surface_splitting_disparity_image[i,j])\n",
    "            vertical_Obj_List.append(appendObjectVert)\n",
    "        \n",
    "        \n",
    "        \n",
    "        elif(current_local_value == 'horizontal'):\n",
    "            horCount += 1\n",
    "            der_val = df_derivative[i, j]\n",
    "            horizontal_Derivative_List.append(der_val)\n",
    "            # Object = [Disparity value, vertical derivative value, i coordinate, j coordinate, label]\n",
    "            appendObjectHor = [surface_splitting_disparity_image[i, j], der_val, i, j, 'horizontal']\n",
    "            horizontal_Disp_List.append(surface_splitting_disparity_image[i,j])\n",
    "            horizontal_Obj_List.append(appendObjectHor)\n",
    "            single_col_obj_list.append(appendObjectHor)\n",
    "\n",
    "        else:\n",
    "            noneCount+=1\n",
    "    divided_horizontal_obj_list.append(single_col_obj_list)\n",
    "\n",
    "print(\"verticals: \", vertCount)\n",
    "print(\"Horizontals: \", horCount)\n",
    "print(\"Unknown: \", noneCount)\n",
    "print(\"[Disparity Value, Derivative Value, V index, U index, Label]\")\n",
    "print(horizontal_Obj_List[202])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vertical_Disp_List is a list of disparity values\n",
    "vertical_Disp_List = np.array(vertical_Disp_List)  # Convert list to NumPy array for efficient operations\n",
    "\n",
    "# Adding higher percentiles (97%, 98%, 99%)\n",
    "p97 = np.percentile(vertical_Disp_List, 97)\n",
    "p3 = np.percentile(vertical_Disp_List, 3)\n",
    "filtered_data_97 = vertical_Disp_List[(vertical_Disp_List >= p3) & (vertical_Disp_List <= p97)]\n",
    "\n",
    "p98 = np.percentile(vertical_Disp_List, 98)\n",
    "p2 = np.percentile(vertical_Disp_List, 2)\n",
    "filtered_data_98 = vertical_Disp_List[(vertical_Disp_List >= p2) & (vertical_Disp_List <= p98)]\n",
    "filtered_data_2 = vertical_Disp_List[(vertical_Disp_List < p2) | (vertical_Disp_List > p98)]\n",
    "\n",
    "p99 = np.percentile(vertical_Disp_List, 99)\n",
    "p1 = np.percentile(vertical_Disp_List, 1)\n",
    "filtered_data_99 = vertical_Disp_List[(vertical_Disp_List >= p1) & (vertical_Disp_List <= p99)]\n",
    "\n",
    "len_97 = len(filtered_data_97)\n",
    "len_98 = len(filtered_data_98)\n",
    "len_99 = len(filtered_data_99)\n",
    "\n",
    "\n",
    "print(\"Length of data capturing 97%: \", len_97)\n",
    "print(\"Length of data capturing 98%: \", len_98)\n",
    "print(\"Length of data capturing 99%: \", len_99)\n",
    "\n",
    "min_97 = min(filtered_data_97)\n",
    "max_97 = max(filtered_data_97)\n",
    "\n",
    "min_98 = min(filtered_data_98)\n",
    "max_98 = max(filtered_data_98)\n",
    "\n",
    "min_99 = min(filtered_data_99)\n",
    "max_99 = max(filtered_data_99)\n",
    "\n",
    "min_2 = min(filtered_data_2)\n",
    "max_2 = max(filtered_data_2)\n",
    "\n",
    "print(\"Min: \", min_97, \" and max: \", max_97, \" for values within 97%\")\n",
    "print(\"Min: \", min_98, \" and max: \", max_98, \" for values within 98%\")\n",
    "print(\"Min \", min_99, \" and max: \", max_99, \" for values within 99%\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Min: \", min_2, \" and max: \", max_2, \" for values outside 98%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "auto_range=(-1,60)\n",
    "def plot_histogram_with_peaks(data, bins, min_prominence=0.01, min_distance=1, auto_range=auto_range):\n",
    "\n",
    "    # Ensure data is cleaned\n",
    "    data = np.array(data)\n",
    "    data = data[np.isfinite(data)]\n",
    "\n",
    "\n",
    "    hist, bin_edges = np.histogram(data,range=auto_range, bins=bins, density=True)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2  # Get bin centers for plotting\n",
    "\n",
    "    # Find peaks with adjusted parameters\n",
    "    peaks, properties = find_peaks(hist, prominence=min_prominence, distance=min_distance)\n",
    "\n",
    "    # Plotting the histogram and peaks\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data, bins=bins,  range=auto_range, alpha=0.6, label='Histogram', density=True)\n",
    "    plt.plot(bin_centers, hist, label='Histogram Line')\n",
    "    plt.scatter(bin_centers[peaks], hist[peaks],color='red', s=100, zorder=5, label='Peaks/Modes')\n",
    "    plt.title('Cluster Data and Detected Modes')\n",
    "    plt.xlabel('Data Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Print peak bin centers\n",
    "    peak_values = bin_centers[peaks]\n",
    "    print(\"Detected modes at:\", peak_values)\n",
    "    return peak_values\n",
    "\n",
    "# Call the function with parameters adjusted based on your description\n",
    "detected_modes = plot_histogram_with_peaks(filtered_data_98, bins=100, min_prominence=0.01, min_distance=1)\n",
    "detected_modes = plot_histogram_with_peaks(vertical_Disp_List, bins=100, min_prominence=0.01, min_distance=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringVerticalSurfaces(flattened_derivatives, a_mini=-0.3, a_maxi=50, clusters=4):\n",
    "    # Clipping the data to be between -0.3 and 0.5\n",
    "    clipped_derivatives = np.clip(flattened_derivatives, a_min=a_mini, a_max=a_maxi)\n",
    "\n",
    "    # Reshape data for KMeans\n",
    "    clustering_model = clipped_derivatives.reshape(-1, 1)    \n",
    "\n",
    "    #kmeans = KMeans(n_clusters=clusters, init=initial_centers, n_init=1)\n",
    "    kmeans = KMeans(n_clusters=clusters, init='k-means++', n_init=10)\n",
    "    kmeans.fit(clustering_model)\n",
    "\n",
    "    # Get the labels and cluster centers\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    return clustering_model, clipped_derivatives, labels, centers\n",
    "\n",
    "num_clusters = len(detected_modes)\n",
    "\n",
    "#model_choice = \"Standard\"\n",
    "#model_choice= \"Simple_98\"\n",
    "model_choice = \"Advanced_98\"\n",
    "\n",
    "# Standard model\n",
    "if(model_choice == \"Standard\"):\n",
    "    clustering_model, clipped_derivatives, labels, centers = clusteringVerticalSurfaces(vertical_Disp_List, clusters=num_clusters)\n",
    "\n",
    "# 98th percentile model\n",
    "if(model_choice = \"Simple_98\"):\n",
    "    clustering_model, clipped_derivatives, labels, centers = clusteringVerticalSurfaces(vertical_Disp_List, clusters=num_clusters, a_mini=min_98, a_maxi=max_98)\n",
    "\n",
    "# 98th percentile + Unknown labelings\n",
    "if(model_choice ==\"Advanced_98\"):\n",
    "    clustering_model, clipped_derivatives, labels, centers = clusteringVerticalSurfaces(filtered_data_98, clusters=num_clusters, a_mini=min_98, a_maxi=max_98)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Separate the data into two clusters\n",
    "print(\"Number og clusters: \", num_clusters)\n",
    "if(num_clusters>= 1):\n",
    "    cluster1 = clustering_model[labels == 0]\n",
    "if(num_clusters>= 2):\n",
    "    cluster2 = clustering_model[labels == 1]\n",
    "if(num_clusters >= 3):\n",
    "    cluster3 = clustering_model[labels == 2]\n",
    "if(num_clusters >= 4):\n",
    "    cluster4 = clustering_model[labels == 3]\n",
    "if(num_clusters >= 5):\n",
    "    cluster5 = clustering_model[labels == 4]\n",
    "\n",
    "\n",
    "# Plot histogram and Gaussian curves\n",
    "plt.figure()\n",
    "if(num_clusters>= 1):\n",
    "    plt.hist(cluster1, bins=200, range=(0, 50), density=True, alpha=0.6, color='blue', edgecolor='blue')\n",
    "if(num_clusters>= 2):\n",
    "    plt.hist(cluster2, bins=200, range=(0, 50), density=True, alpha=0.6, color='red', edgecolor='red')\n",
    "if(num_clusters>= 3):\n",
    "    plt.hist(cluster3, bins=200, range=(0, 50), density=True, alpha=0.6, color='green', edgecolor='green')\n",
    "if(num_clusters>= 4):\n",
    "    plt.hist(cluster4, bins=200, range=(0, 50), density=True, alpha=0.6, color='yellow', edgecolor='yellow')\n",
    "if(num_clusters>= 5):\n",
    "    plt.hist(cluster5, bins=200, range=(0, 50), density=True, alpha=0.6, color='purple', edgecolor='purple')\n",
    "#plt.plot(x_values, gaussian_curve1, label='Gaussian 1')\n",
    "#plt.plot(x_values, gaussian_curve2, label='Gaussian 2')\n",
    "plt.xlabel('Derivative Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram with Fitted Gaussian Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Create classmap\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for i, obj in enumerate(vertical_Obj_List):\n",
    "    if(labels[i] == 0):\n",
    "        obj[3] = \"vertical1\"\n",
    "    elif(labels[i] == 1):\n",
    "        obj[3] = \"vertical2\"\n",
    "    elif(labels[i] == 2):\n",
    "        obj[3] = \"vertical3\"\n",
    "    elif(labels[i] == 3):\n",
    "        obj[3] = \"vertical4\"\n",
    "    elif(labels[i]==4):\n",
    "        obj[3] = \"vertical5\"\n",
    "    else:\n",
    "        obj[3] = \"unknown\"\n",
    "\n",
    "\n",
    "new_class_map = np.copy(classification_map)\n",
    "\n",
    "for i, obj in enumerate(vertical_Obj_List):\n",
    "    i = obj[1]\n",
    "    j= obj[2]\n",
    "    new_class_map[i,j] = obj[3]\n",
    "\n",
    "\n",
    "\"\"\" # Dark color definitions with brightness < 128/256\n",
    "# Brightness = 0.18\n",
    "Navy_Blue = [0, 0, 128/256]  # Brightness = 0.17\n",
    "Dark_Slate_Gray = [47/256, 79/256, 79/256]  # Brightness = 0.32\n",
    "Dark_Cyan = [0, 139/256, 139/256]  # Brightness = 0.18\n",
    "Dark_Goldenrod = [184/256, 134/256, 11/256]  # Adjusted to be darker\n",
    "White= [1,1,1]\n",
    "\n",
    "# Revised dark, distinct color definitions\n",
    "Dark_Red = [139/256, 0, 0]  # Rich, dark red\n",
    "Dark_Slate_Gray = [47/256, 79/256, 79/256]\n",
    "Sienna = [160/256, 82/256, 45/256]  # Earthy, medium dark\n",
    "Dark_Orange = [255/256, 140/256, 0]  # Vibrant, but darkened\n",
    "Saddle_Brown = [139/256, 69/256, 19/256] \n",
    "Pure_Red =[200/256,0,0]\n",
    "\n",
    "# Horizontal color definitions\n",
    "Indigo = [75/256, 0, 130/256]  # Deep indigo blue\n",
    "Teal = [0, 128/256, 128/256]  # Rich teal blue\n",
    "Forest_Green = [34/256, 139/256, 34/256]  # Deep forest green\n",
    "Dark_Olive_Green = [85/256, 107/256, 47/256]  # Muted, darker green\n",
    "Purple = [128/256, 0, 128/256]  # Rich deep purple\n",
    "Midnight_Blue = [25/256, 25/256, 112/256] \n",
    "Dark_Green = [0, 100/256, 0]  \n",
    "Pure_Green = [0,200/256,0]\n",
    "Pure_Blue = [0,0,200/256]\n",
    "\n",
    "\n",
    "# Updated color scheme for classification ensuring maximum distinction\n",
    "colorScheme = {\n",
    "    'horizontal1': Navy_Blue,\n",
    "    'horizontal2': Teal,\n",
    "    'horizontal3': Pure_Green,\n",
    "    'horizontal4': Indigo,\n",
    "    'horizontal5': Dark_Olive_Green,\n",
    "    'horizontal6': Pure_Green,  # Custom darker teal\n",
    "    'horizontal7': Dark_Green,  # Custom dark leaf green\n",
    "    'vertical1': Dark_Red,\n",
    "    'vertical2': Pure_Red,\n",
    "    'vertical3': Sienna,\n",
    "    'vertical4': Dark_Orange,\n",
    "    'vertical5': Dark_Slate_Gray,  # Custom burnt orange\n",
    "    'unknown': White,  # White\n",
    "    'undecided': White,  # White\n",
    "    'upright': Pure_Red,\n",
    "    'horizontal': Pure_Green\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "# Create an empty array for the color-coded image\n",
    "color_coded_image = np.zeros((*new_class_map.shape, 3))\n",
    "#print(color_coded_image)\n",
    "\n",
    "\n",
    "color_coded_image = np.zeros((*new_class_map.shape, 3))\n",
    "\n",
    "# Assign colors based on the classification in new_class_map\n",
    "for label, color in colorScheme.items():\n",
    "    # Find indices where the current label is present in new_class_map\n",
    "    indices = np.where(new_class_map == label)\n",
    "    # Assign the corresponding color to these indices in the color-coded image\n",
    "    color_coded_image[indices] = color\n",
    "\n",
    "#print(color_coded_image)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(color_coded_image)\n",
    "plt.title('Pixel-wise Disparity Map Classification')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster1: \", (cluster1))\n",
    "print(\"2 percent list: \",type(filtered_data_2))\n",
    "print(\"Labels: \", (labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endre/change/velg\n",
    "# Velg column index here\n",
    "column_index = 500\n",
    "\n",
    "#Total rows (v) up = 1080\n",
    "#Total columns (u) right = 1920\n",
    "\n",
    "# Endre/change/velg\n",
    "# Velg column data here\n",
    "column_data = surface_splitting_disparity_image[:, column_index]\n",
    "y_values = range(len(column_data))\n",
    "\n",
    "\n",
    "\n",
    "print(surface_splitting_disparity_image[:, column_index])\n",
    "print(df_derivative[:, column_index])\n",
    "print(classification_map[:, column_index])\n",
    "\n",
    "count = 0\n",
    "#horizontal_column_disparity_list = []\n",
    "\n",
    "\n",
    "\n",
    "def single_column_list_generator(column_index=column_index, only_return_hor_col=False):\n",
    "\n",
    "    horizontal_column_derivative_list = []\n",
    "    horizontal_column_object_list = []\n",
    "    vertical_column_derivative_list = []\n",
    "    vertical_column_object_list = []\n",
    "    for i, val in enumerate(classification_map[:, column_index]):\n",
    "        #print(val)\n",
    "        \n",
    "        if(val == 'horizontal'):\n",
    "            der = df_derivative[i, column_index]\n",
    "            disp = surface_splitting_disparity_image[i, column_index]\n",
    "\n",
    "            appendObjColHor = [der, disp, i, column_index, val]\n",
    "            horizontal_column_derivative_list.append(der)\n",
    "            horizontal_column_object_list.append(appendObjColHor)\n",
    "\n",
    "        #if(val == 'vertical1' or val == 'vertical2' or val == 'vertical3' or val == 'vertical4'):\n",
    "        if(val == 'upright'):\n",
    "            der = df_derivative[i, column_index]\n",
    "            disp = surface_splitting_disparity_image[i, column_index]\n",
    "\n",
    "            appendObjColVert = [der, disp, i, column_index, val]\n",
    "            vertical_column_derivative_list.append(der)\n",
    "            vertical_column_object_list.append(appendObjColVert)\n",
    "    if(only_return_hor_col):\n",
    "        return horizontal_column_object_list\n",
    "    return horizontal_column_derivative_list, horizontal_column_object_list, vertical_column_derivative_list, vertical_column_object_list\n",
    "\n",
    "horizontal_column_derivative_list, horizontal_column_object_list, vertical_column_derivative_list, vertical_column_object_list = single_column_list_generator()\n",
    "\n",
    "def generate_index_list(column_object_list=horizontal_column_object_list):\n",
    "    indexListColumn = []\n",
    "    for i in range(len(column_object_list)):\n",
    "        ind = column_object_list[i][2]\n",
    "        indexListColumn.append(ind)\n",
    "    return indexListColumn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Index list horCOl\n",
    "print(len(horizontal_column_object_list))\n",
    "indexListHorCol = []\n",
    "for i in range(len(horizontal_column_object_list)):\n",
    "    ind = horizontal_column_object_list[i][2]\n",
    "    indexListHorCol.append(ind)\n",
    "    #print(\"Index \", ind)\n",
    "print(\"IndexListHorCol: \",indexListHorCol)\n",
    "\n",
    "\n",
    "# Index list vertCol\n",
    "print(len(vertical_column_object_list))\n",
    "indexListVertCol = []\n",
    "for i in range(len(vertical_column_object_list)):\n",
    "    ind = vertical_column_object_list[i][2]\n",
    "    indexListVertCol.append(ind)\n",
    "    #print(\"Index \", ind)\n",
    "print(\"IndexListVertCol: \",indexListVertCol)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "#plt.plot(y_values, column_data)\n",
    "\n",
    "highlighted_horizontal_y_values = [y_values[i] for i in indexListHorCol]\n",
    "highlighted_horizontal_column_data = [column_data[i] for i in indexListHorCol]\n",
    "\n",
    "highlighted_vertical_y_values = [y_values[i] for i in indexListVertCol]\n",
    "highlighted_vertical_column_data = [column_data[i] for i in indexListVertCol]\n",
    "\n",
    "\n",
    "\n",
    "# Plotting highlighted points on top with a different color\n",
    "plt.scatter(highlighted_vertical_y_values, highlighted_vertical_column_data, color='red', s=2, zorder=1, label='Highlighted Points')\n",
    "plt.scatter(highlighted_horizontal_y_values, highlighted_horizontal_column_data, color='green', s=2, zorder=1, label='Highlighted Points')\n",
    "\n",
    "title_for_plot= 'Column index: ' + str(column_index)\n",
    "plt.gca()\n",
    "plt.ylabel('Disparity Value')\n",
    "plt.xlabel('Image v-Axis')\n",
    "plt.title(title_for_plot)\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_dark_colors(n):\n",
    "    # Define the saturation and value for dark colors\n",
    "    saturation = 1.0  # full saturation for vivid colors\n",
    "    value = 0.6  # dark enough to ensure good contrast on a white background\n",
    "    \n",
    "    # Generate colors\n",
    "    colors = []\n",
    "    for i in range(n):\n",
    "        hue = i / n  # Evenly space hues around the color wheel\n",
    "        rgb = mcolors.hsv_to_rgb((hue, saturation, value))\n",
    "        colors.append(rgb)\n",
    "    \n",
    "    return colors\n",
    "\n",
    "# Example usage: generate 10 dark, distinct colors\n",
    "num_colors = 10\n",
    "dark_colors = generate_dark_colors(num_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_indices(indices, limit=10):\n",
    "    if not indices:\n",
    "        return []\n",
    "    sorted_indices = sorted(indices)\n",
    "    groups = [[sorted_indices[0]]]\n",
    "    for index in sorted_indices[1:]:\n",
    "        if index - groups[-1][-1] <= limit:\n",
    "            groups[-1].append(index)\n",
    "        else:\n",
    "            groups.append([index])\n",
    "    return groups\n",
    "\n",
    "def perform_linear_regression(groups, y_values, column_data):\n",
    "    line_equations = []\n",
    "    edge_boundaries = []  # List to store edge boundaries for each group\n",
    "    min_size = 10\n",
    "    for group in groups:\n",
    "        if len(group) >= min_size:\n",
    "            group_y_values = np.array([y_values[idx] for idx in group])\n",
    "            group_column_data = np.array([column_data[idx] for idx in group])\n",
    "            A = np.vstack([group_y_values, np.ones(len(group_y_values))]).T\n",
    "            m, c = np.linalg.lstsq(A, group_column_data, rcond=None)[0]\n",
    "            line_equations.append((m, c))\n",
    "            # Store the min and max v-values for the group\n",
    "            edge_boundaries.append((min(group_y_values), max(group_y_values)))\n",
    "        else:\n",
    "            line_equations.append(None)\n",
    "            edge_boundaries.append(None)\n",
    "    return line_equations, edge_boundaries\n",
    "\n",
    "def reclassify_points_and_identify_edges(indexList, column_data, line_equations, edge_boundaries, column_index):\n",
    "    reclassified_points = []\n",
    "    edge_points = {}\n",
    "    corrected_points = []  # New list to store corrected points\n",
    "    unknown_points = []\n",
    "\n",
    "    for idx, v_value in enumerate(indexList):\n",
    "        u_value = column_index\n",
    "        disparity = column_data[v_value]\n",
    "        \n",
    "        # Function to calculate distance to line\n",
    "        def distance_to_line(line):\n",
    "            m, c = line\n",
    "            return abs(m * v_value - disparity + c) / np.sqrt(m**2 + 1)\n",
    "        \n",
    "        # Create a sorted list of lines based on their distance to the point\n",
    "        valid_lines = [(line, boundaries) for line, boundaries in zip(line_equations, edge_boundaries) if line is not None and boundaries is not None]\n",
    "        sorted_lines = sorted(valid_lines, key=lambda x: distance_to_line(x[0]))\n",
    "\n",
    "        # Try to classify the point to the first valid line within boundaries\n",
    "        classified = False\n",
    "        for line, (min_v, max_v) in sorted_lines:\n",
    "            if min_v <= v_value <= max_v:\n",
    "                reclassified_points.append((u_value, v_value, disparity, line))\n",
    "                m, c = line\n",
    "                corrected_disparity = m * v_value + c  # y = mx + c\n",
    "                corrected_points.append((u_value, v_value, corrected_disparity, line))\n",
    "                classified = True\n",
    "                break\n",
    "\n",
    "        # Update edge points if the point was classified\n",
    "        if classified:\n",
    "            if line not in edge_points:\n",
    "                edge_points[line] = {'min_v': (float('inf'), None), 'max_v': (float('-inf'), None)}\n",
    "            if v_value < edge_points[line]['min_v'][0]:\n",
    "                edge_points[line]['min_v'] = (v_value, (u_value, v_value, disparity))\n",
    "            if v_value > edge_points[line]['max_v'][0]:\n",
    "                edge_points[line]['max_v'] = (v_value, (u_value, v_value, disparity))\n",
    "        else:\n",
    "            # If no valid line was found to classify the point, add it to the unknown points list\n",
    "            unknown_points.append((u_value, v_value, disparity, \"unknown\"))\n",
    "\n",
    "    return reclassified_points, edge_points, corrected_points, unknown_points\n",
    "\n",
    "\n",
    "size_of_fig=(10, 6)\n",
    "# Main function to process column data\n",
    "def process_column(column_index, plot=False, size_of_fig=size_of_fig, group_limit=10):\n",
    "    column_data = surface_splitting_disparity_image[:, column_index]\n",
    "    y_values = range(len(column_data))\n",
    "\n",
    "    horizontal_column_object_list = single_column_list_generator(column_index, True)\n",
    "    indexListHorCol = generate_index_list(horizontal_column_object_list)\n",
    "\n",
    "\n",
    "    groups = group_indices(indexListHorCol, limit=group_limit)\n",
    "    colors = generate_dark_colors(len(groups))\n",
    "\n",
    "    \n",
    "    line_equations, edge_boundaries = perform_linear_regression(groups, list(y_values), column_data)\n",
    "    reclassified_points, edge_points, corrected_points, unknown_points = reclassify_points_and_identify_edges(indexListHorCol, column_data, line_equations, edge_boundaries, column_index)\n",
    "\n",
    "    if plot:\n",
    "        # Plot 1: Grouping and Regression\n",
    "        plt.figure(figsize=size_of_fig)\n",
    "        for i, group in enumerate(groups):\n",
    "            if group:\n",
    "                group_y_values = [y_values[idx] for idx in group]\n",
    "                group_column_data = [column_data[idx] for idx in group]\n",
    "                plt.scatter(group_y_values, group_column_data, color=colors[i], s=10, label=f'Group {i+1}')\n",
    "                \n",
    "                if line_equations[i]:\n",
    "                    m, c = line_equations[i]\n",
    "                    # Calculate extended x values but make them shorter than the full range\n",
    "                    extended_start = min(group_y_values) - 100  # you can adjust these values to control the length\n",
    "                    extended_end = max(group_y_values) + 100   # likewise, adjust here for the end\n",
    "                    extended_x = np.linspace(extended_start, extended_end, 400)\n",
    "                    plt.plot(extended_x, m * extended_x + c, color=colors[i], label=f'Line: y={m:.2f}x+{c:.2f}')\n",
    "\n",
    "        plt.xlabel('Image v-Axis')\n",
    "        plt.ylabel('Disparity Value')\n",
    "        plt.title('Grouping and Regression for Column ' + str(column_index))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot 2: Reclassified Points\n",
    "        line_to_color = {line: colors[i] for i, line in enumerate(line_equations) if line is not None}\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for point in reclassified_points:\n",
    "            u_value, v_value, disparity, line = point\n",
    "            if line in line_to_color:\n",
    "                color = line_to_color[line]\n",
    "                plt.scatter(v_value, disparity, color=color, s=10)  # Use the color mapped to the line\n",
    "\n",
    "        for point in unknown_points:\n",
    "            u_value, v_value, disparity, line = point\n",
    "            plt.scatter(v_value, disparity, color='black', s=10)\n",
    "\n",
    "        plt.xlabel('Image v-Axis')\n",
    "        plt.ylabel('Disparity Value')\n",
    "        plt.title('Reclassified Points for Column ' + str(column_index))\n",
    "        plt.show()\n",
    "\n",
    "        # Plot 3: Edge Points\n",
    "        plt.figure(figsize=size_of_fig)\n",
    "        line_to_color = {line: colors[i] for i, line in enumerate(line_equations) if line is not None}\n",
    "        for line, edge in edge_points.items():\n",
    "            if line in line_to_color:\n",
    "                color = line_to_color[line]  # Use the mapped color for the line\n",
    "                for key, value in edge.items():\n",
    "                    if value[1] is not None:\n",
    "                        plt.scatter(value[1][1], value[1][2], color=color, s=10, label=f'{key} Edge Point')\n",
    "        for point in unknown_points:\n",
    "            u_value, v_value, disparity, line = point\n",
    "            plt.scatter(v_value, disparity, color='black', s=10)\n",
    "        plt.xlabel('Image v-Axis')\n",
    "        plt.ylabel('Disparity Value')\n",
    "        plt.title('Edge Points for Column ' + str(column_index))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "   \n",
    "        # Plot corrected points as an additional plot or integrated into one of the existing plots\n",
    "        plt.figure(figsize=size_of_fig)\n",
    "        for point in corrected_points:\n",
    "            u_value, v_value, disparity, line = point\n",
    "            if line in line_to_color:\n",
    "                color = line_to_color[line]\n",
    "                plt.scatter(v_value, disparity, color=color, s=10) \n",
    "        for point in unknown_points:\n",
    "            u_value, v_value, disparity, line = point\n",
    "            plt.scatter(v_value, disparity, color='black', s=10)\n",
    "        plt.xlabel('Image v-Axis')\n",
    "        plt.ylabel('Corrected Disparity Value')\n",
    "        plt.title('Corrected Points for Column ' + str(column_index))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return reclassified_points, edge_points, corrected_points, unknown_points\n",
    "\n",
    "\n",
    "def group_corrected_points_by_line(corrected_points, unknown_points):\n",
    "    line_groups = {}\n",
    "    for point in corrected_points:\n",
    "        u_value, v_value, corrected_disparity, line = point\n",
    "        line_key = tuple(line)  # Use the line equation as the dictionary key\n",
    "        if line_key not in line_groups:\n",
    "            line_groups[line_key] = []\n",
    "        line_groups[line_key].append([u_value, v_value, corrected_disparity])\n",
    "\n",
    "    # Handle unknown points\n",
    "    unknown_key = ('unknown',)\n",
    "    line_groups[unknown_key] = []\n",
    "    for point in unknown_points:\n",
    "        u_value, v_value, disparity, _ = point\n",
    "        line_groups[unknown_key].append([u_value, v_value, disparity])\n",
    "\n",
    "    return list(line_groups.values())\n",
    "\n",
    "\n",
    "\n",
    "horizontal_label_list = ['horizontal1', 'horizontal2', 'horizontal3', 'horizontal4', 'horizontal5', 'horizontal6', 'horizontal7']\n",
    "\n",
    "def new_column_object_list_generator(grouped_corrected_points, new_full_object_list=[]):\n",
    "\n",
    "    new_column_object_list = []\n",
    "\n",
    "    last_group = len(grouped_corrected_points)-1\n",
    "    #print(last_group)\n",
    "\n",
    "    for i, group in enumerate(reversed(grouped_corrected_points)):\n",
    "        for point in group:\n",
    "            if(i>= len(horizontal_label_list) or (i ==0)):\n",
    "                point.append('unknown')\n",
    "            else:\n",
    "                point.append(horizontal_label_list[i-1])\n",
    "            #print(\"Point: \", point)\n",
    "            new_column_object_list.append(point)\n",
    "            new_full_object_list.append(point)\n",
    "\n",
    "    return new_column_object_list, new_full_object_list\n",
    "\n",
    "# Example of how to call this function\n",
    "current_column_index = 1500  # Example column index\n",
    "reclassified_points_initial, edge_points_initial, corrected_points, unknown_points =  process_column(current_column_index, plot=True, group_limit=20)\n",
    "\n",
    "grouped_corrected_points = group_corrected_points_by_line(corrected_points, unknown_points)\n",
    "newest_test_list, disregarded= new_column_object_list_generator(grouped_corrected_points)\n",
    "\n",
    "#print(\"Length \",len(newest_test_list))\n",
    "\n",
    "\n",
    "for i, group in enumerate(grouped_corrected_points):\n",
    "    print(\"index: \", i)\n",
    "    print(\"Group:\", group) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 15 secs to run\n",
    "def all_columns_full_horizontal_object_list_generator(surface_splitting_disparity_image, plot=False, group_limit=10):\n",
    "    all_columns_full_horizontal_object_list = []\n",
    "    \n",
    "    # Assume there is a known number of columns\n",
    "    total_columns = surface_splitting_disparity_image.shape[1]\n",
    "\n",
    "    for current_column_index in range(total_columns):\n",
    "        reclassified_points_initial, edge_points_initial, corrected_points, unknown_points =  process_column(current_column_index, plot=False,group_limit=group_limit)\n",
    "        grouped_corrected_points = group_corrected_points_by_line(corrected_points, unknown_points)\n",
    "        newest_test_list, all_columns_full_horizontal_object_list = new_column_object_list_generator(grouped_corrected_points, all_columns_full_horizontal_object_list)\n",
    "        \n",
    "    return all_columns_full_horizontal_object_list\n",
    "\n",
    "all_columns_full_horizontal_object_list = all_columns_full_horizontal_object_list_generator(surface_splitting_disparity_image, group_limit=20)\n",
    "print(\"Length all columns\",len(all_columns_full_horizontal_object_list))\n",
    "print(\"Length obj list\", len(horizontal_Obj_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_test_class_map = np.copy(classification_map)\n",
    "FULL_TEST_CLASS_MAP = np.copy(new_class_map)\n",
    "\n",
    "\n",
    "for index, obj in enumerate(all_columns_full_horizontal_object_list):\n",
    "    i = obj[1]\n",
    "    j= obj[0]\n",
    "    lab = obj[3]   \n",
    "    horizontal_test_class_map[i,j] = obj[3]\n",
    "    FULL_TEST_CLASS_MAP[i, j] = obj[3]\n",
    "\"\"\" \n",
    "# Dark color definitions with brightness < 128/256\n",
    "# Brightness = 0.18\n",
    "Navy_Blue = [0, 0, 128/256]  # Brightness = 0.17\n",
    "Dark_Slate_Gray = [47/256, 79/256, 79/256]  # Brightness = 0.32\n",
    "Dark_Cyan = [0, 139/256, 139/256]  # Brightness = 0.18\n",
    "Dark_Goldenrod = [184/256, 134/256, 11/256]  # Adjusted to be darker\n",
    "White= [1,1,1]\n",
    "\n",
    "# Revised dark, distinct color definitions\n",
    "Dark_Red = [139/256, 0, 0]  # Rich, dark red\n",
    "Dark_Slate_Gray = [47/256, 79/256, 79/256]\n",
    "Sienna = [160/256, 82/256, 45/256]  # Earthy, medium dark\n",
    "Dark_Orange = [255/256, 140/256, 0]  # Vibrant, but darkened\n",
    "Saddle_Brown = [139/256, 69/256, 19/256] \n",
    "Pure_Red =[200/256,0,0]\n",
    "\n",
    "# Horizontal color definitions\n",
    "Indigo = [75/256, 0, 130/256]  # Deep indigo blue\n",
    "Teal = [0, 128/256, 128/256]  # Rich teal blue\n",
    "Forest_Green = [34/256, 139/256, 34/256]  # Deep forest green\n",
    "Dark_Olive_Green = [85/256, 107/256, 47/256]  # Muted, darker green\n",
    "Midnight_Blue = [25/256, 25/256, 112/256] \n",
    "Dark_Green = [0, 100/256, 0]  \n",
    "Pure_Green = [0,200/256,0]\n",
    "Pure_Blue = [0,0,200/256]\n",
    "\n",
    "\n",
    "# Updated color scheme for classification ensuring maximum distinction\n",
    "colorScheme = {\n",
    "    'horizontal1': Navy_Blue,\n",
    "    'horizontal2': Teal,\n",
    "    'horizontal3': Pure_Green,\n",
    "    'horizontal4': Indigo,\n",
    "    'horizontal5': Dark_Olive_Green,\n",
    "    'horizontal6': Pure_Green,  # Custom darker teal\n",
    "    'horizontal7': Dark_Green,  # Custom dark leaf green\n",
    "    'vertical1': Dark_Red,\n",
    "    'vertical2': Pure_Red,\n",
    "    'vertical3': Sienna,\n",
    "    'vertical4': Dark_Orange,\n",
    "    'vertical5': Dark_Slate_Gray,  # Custom burnt orange\n",
    "    'unknown': White,  # White\n",
    "    'undecided': White,  # White\n",
    "    'upright': Pure_Red,\n",
    "    'horizontal': Pure_Blue\n",
    "}\n",
    " \"\"\"\n",
    "\n",
    "# Create an empty array for the color-coded image\n",
    "horizontal_color_coded_image = np.zeros((*horizontal_test_class_map.shape, 3))\n",
    "#print(color_coded_image)\n",
    "full_color_coded_image = np.zeros((*FULL_TEST_CLASS_MAP.shape, 3))\n",
    "\n",
    "\n",
    "# Assign colors based on the classification in new_class_map\n",
    "for label, color in colorScheme.items():\n",
    "    # Find indices where the current label is present in new_class_map\n",
    "    indices = np.where(horizontal_test_class_map == label)\n",
    "    fulldices= np.where(FULL_TEST_CLASS_MAP==label)\n",
    "    # Assign the corresponding color to these indices in the color-coded image\n",
    "    horizontal_color_coded_image[indices] = color\n",
    "    full_color_coded_image[fulldices] = color\n",
    "\n",
    "#print(color_coded_image)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(horizontal_color_coded_image)\n",
    "plt.title('Pixel-wise Disparity Map Classification')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show() \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(color_coded_image)\n",
    "plt.title('Pixel-wise Disparity Map Classification')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show() \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(full_color_coded_image)\n",
    "plt.title('Pixel-wise Disparity Map Classification')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show() \n",
    "\n",
    "display_disp_image(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TELEPORTER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
